{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bayesian Active Learning with Image Data\n",
    "\n",
    "This is an implementation of the paper Deep Bayesian Active Learning with Image Data using keras and modAL. [modAL](https://modal-python.readthedocs.io/en/latest/) is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
    "\n",
    "## Active Learning\n",
    "\n",
    "In this notebook, we are concerned with pool-based Active Learning. In this setting, we have a large amount of unlabelled data and a small initial labelled training set and we want to choose what data should be labelled next.\n",
    "\n",
    "To do so, there are several query strategies. In this notebook, we will be using uncertainty sampling: the data chosen to be annotated is the one that maximizes an uncertainty criterion (entropy, gini index, variation ratios ...). \n",
    "\n",
    "## Dropout-Based Bayesian Deep Neural Networks\n",
    "\n",
    "In this Notebook, we will select the data from the unlabelled pool that maximizes the uncertainty of our model. But the model we will be using will be a Bayesian Deep Neural Network. \n",
    "\n",
    "Unlike Traditional Deep Learning, where we are looking for the set of weights that maximizes the likelihood of the data (MLE), in bayesian deep learning we are looking for the posterior distribution over the weights and the prediction is then obtained by marginalizing out the weights. As a result, Bayesian models are less prone to overfitting. But unfortunately for big deep models, the posterior distribution is intractable, and we need approximations. \n",
    "\n",
    "In 2015, [Gal and Ghahramani](https://arxiv.org/pdf/1506.02142.pdf) showed that deep models with dropout layers can be viewed as a lightweight bayesian approximation. The prior and posterior distributions are simply Bernoulli distributions (0 or the learned value). And the predictions can be cheaply obtained at test time by performing Monte Carlo integrations with dropout layers activated.\n",
    "\n",
    "So in a nutshell, Dropout-based Bayesian Neural Nets are simply Neural Nets with Dropout layers activated at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(create_keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28, 28, 1).astype('float32') / 255.\n",
    "X_test = X_test.reshape(10000, 28, 28, 1).astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_idx = np.array([],dtype=np.int)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train[:,i]==1)[0], size=2, replace=False)\n",
    "    initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial unlabelled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "y_pool = np.delete(y_train, initial_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(learner, X, n_instances=1):\n",
    "    query_idx = np.random.choice(range(len(X)), size=n_instances, replace=False)\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_entropy(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
    "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
    "                           [learner.estimator.model.layers[-1].output])\n",
    "    learning_phase = True\n",
    "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
    "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
    "    expected_p = np.mean(MC_samples, axis=0)\n",
    "    acquisition = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_ratio(learner, X, n_instances=1, T=100):\n",
    "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
    "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
    "                           [learner.estimator.model.layers[-1].output])\n",
    "    learning_phase = True\n",
    "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
    "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
    "    preds = np.argmax(a, axis=2)\n",
    "    mode, count = stats.mode(preds, axis=0)\n",
    "    acquisition = (1 - count / preds.shape[1]).reshape((-1,))\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bald(learner, X, n_instances, T=100):\n",
    "    random_subset = np.random.choice(X.shape[0], 2000, replace=False)\n",
    "    MC_output = K.function([learner.estimator.model.layers[0].input, K.learning_phase()],\n",
    "                           [learner.estimator.model.layers[-1].output])\n",
    "    learning_phase = True\n",
    "    MC_samples = [MC_output([X[random_subset], learning_phase])[0] for _ in range(T)]\n",
    "    MC_samples = np.array(MC_samples)  # [#samples x batch size x #classes]\n",
    "    expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "    expected_p = np.mean(MC_samples, axis=0)\n",
    "    entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "    acquisition = entropy_expected_p - expected_entropy\n",
    "    idx = (-acquisition).argsort()[:n_instances]\n",
    "    query_idx = random_subset[idx]\n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure(query_strategy,\n",
    "                              X_test,\n",
    "                              y_test,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              estimator,\n",
    "                              epochs=50,\n",
    "                              batch_size=128,\n",
    "                              n_queries=100,\n",
    "                              n_instances=10,\n",
    "                              verbose=0):\n",
    "    learner = ActiveLearner(estimator=estimator,\n",
    "                            X_training=X_initial,\n",
    "                            y_training=y_initial,\n",
    "                            query_strategy=query_strategy,\n",
    "                            verbose=verbose\n",
    "                           )\n",
    "    perf_hist = [learner.score(X_test, y_test, verbose=verbose)]\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = learner.query(X_pool, n_instances)\n",
    "        learner.teach(X_pool[query_idx], y_pool[query_idx], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        model_accuracy = learner.score(X_test, y_test, verbose=0)\n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "    return perf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.5734\n",
      "Accuracy after query 2: 0.6242\n",
      "Accuracy after query 3: 0.7004\n",
      "Accuracy after query 4: 0.7202\n",
      "Accuracy after query 5: 0.7447\n",
      "Accuracy after query 6: 0.7538\n",
      "Accuracy after query 7: 0.7601\n",
      "Accuracy after query 8: 0.7568\n",
      "Accuracy after query 9: 0.7777\n",
      "Accuracy after query 10: 0.7962\n",
      "Accuracy after query 11: 0.7513\n",
      "Accuracy after query 12: 0.8389\n",
      "Accuracy after query 13: 0.8171\n",
      "Accuracy after query 14: 0.8451\n",
      "Accuracy after query 15: 0.8963\n",
      "Accuracy after query 16: 0.8713\n",
      "Accuracy after query 17: 0.8902\n",
      "Accuracy after query 18: 0.8973\n",
      "Accuracy after query 19: 0.9101\n",
      "Accuracy after query 20: 0.9119\n",
      "Accuracy after query 21: 0.8986\n",
      "Accuracy after query 22: 0.9097\n",
      "Accuracy after query 23: 0.9086\n",
      "Accuracy after query 24: 0.9140\n",
      "Accuracy after query 25: 0.9228\n",
      "Accuracy after query 26: 0.9241\n",
      "Accuracy after query 27: 0.9273\n",
      "Accuracy after query 28: 0.9204\n",
      "Accuracy after query 29: 0.9253\n",
      "Accuracy after query 30: 0.9424\n",
      "Accuracy after query 31: 0.9436\n",
      "Accuracy after query 32: 0.9473\n",
      "Accuracy after query 33: 0.9502\n",
      "Accuracy after query 34: 0.9473\n",
      "Accuracy after query 35: 0.9466\n",
      "Accuracy after query 36: 0.9569\n",
      "Accuracy after query 37: 0.9340\n",
      "Accuracy after query 38: 0.9568\n",
      "Accuracy after query 39: 0.9414\n",
      "Accuracy after query 40: 0.9566\n",
      "Accuracy after query 41: 0.9644\n",
      "Accuracy after query 42: 0.9642\n",
      "Accuracy after query 43: 0.9551\n",
      "Accuracy after query 44: 0.9607\n",
      "Accuracy after query 45: 0.9625\n",
      "Accuracy after query 46: 0.9642\n",
      "Accuracy after query 47: 0.9620\n",
      "Accuracy after query 48: 0.9579\n",
      "Accuracy after query 49: 0.9679\n",
      "Accuracy after query 50: 0.9614\n",
      "Accuracy after query 51: 0.9693\n",
      "Accuracy after query 52: 0.9678\n",
      "Accuracy after query 53: 0.9683\n",
      "Accuracy after query 54: 0.9721\n",
      "Accuracy after query 55: 0.9689\n",
      "Accuracy after query 56: 0.9665\n",
      "Accuracy after query 57: 0.9697\n",
      "Accuracy after query 58: 0.9733\n",
      "Accuracy after query 59: 0.9727\n",
      "Accuracy after query 60: 0.9679\n",
      "Accuracy after query 61: 0.9726\n",
      "Accuracy after query 62: 0.9721\n",
      "Accuracy after query 63: 0.9641\n",
      "Accuracy after query 64: 0.9729\n",
      "Accuracy after query 65: 0.9749\n",
      "Accuracy after query 66: 0.9732\n",
      "Accuracy after query 67: 0.9740\n",
      "Accuracy after query 68: 0.9718\n",
      "Accuracy after query 69: 0.9741\n",
      "Accuracy after query 70: 0.9767\n",
      "Accuracy after query 71: 0.9758\n",
      "Accuracy after query 72: 0.9786\n",
      "Accuracy after query 73: 0.9729\n",
      "Accuracy after query 74: 0.9772\n",
      "Accuracy after query 75: 0.9777\n",
      "Accuracy after query 76: 0.9782\n",
      "Accuracy after query 77: 0.9766\n",
      "Accuracy after query 78: 0.9816\n",
      "Accuracy after query 79: 0.9771\n",
      "Accuracy after query 80: 0.9783\n",
      "Accuracy after query 81: 0.9811\n",
      "Accuracy after query 82: 0.9818\n",
      "Accuracy after query 83: 0.9799\n",
      "Accuracy after query 84: 0.9809\n",
      "Accuracy after query 85: 0.9789\n",
      "Accuracy after query 86: 0.9803\n",
      "Accuracy after query 87: 0.9801\n",
      "Accuracy after query 88: 0.9792\n",
      "Accuracy after query 89: 0.9813\n",
      "Accuracy after query 90: 0.9827\n",
      "Accuracy after query 91: 0.9827\n",
      "Accuracy after query 92: 0.9832\n",
      "Accuracy after query 93: 0.9839\n",
      "Accuracy after query 94: 0.9801\n",
      "Accuracy after query 95: 0.9826\n",
      "Accuracy after query 96: 0.9811\n",
      "Accuracy after query 97: 0.9817\n",
      "Accuracy after query 98: 0.9834\n",
      "Accuracy after query 99: 0.9843\n",
      "Accuracy after query 100: 0.9825\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(create_keras_model)\n",
    "entropy_perf_hist = active_learning_procedure(max_entropy,\n",
    "                                              X_test,\n",
    "                                              y_test,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              estimator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after query 1: 0.6060\n",
      "Accuracy after query 2: 0.6537\n",
      "Accuracy after query 3: 0.7004\n",
      "Accuracy after query 4: 0.7308\n",
      "Accuracy after query 5: 0.7513\n",
      "Accuracy after query 6: 0.7800\n",
      "Accuracy after query 7: 0.7850\n",
      "Accuracy after query 8: 0.7974\n",
      "Accuracy after query 9: 0.8412\n",
      "Accuracy after query 10: 0.8440\n",
      "Accuracy after query 11: 0.8126\n",
      "Accuracy after query 12: 0.8447\n",
      "Accuracy after query 13: 0.8616\n",
      "Accuracy after query 14: 0.8714\n",
      "Accuracy after query 15: 0.8841\n",
      "Accuracy after query 16: 0.8793\n",
      "Accuracy after query 17: 0.8703\n",
      "Accuracy after query 18: 0.8725\n",
      "Accuracy after query 19: 0.8834\n",
      "Accuracy after query 20: 0.8881\n",
      "Accuracy after query 21: 0.8805\n",
      "Accuracy after query 22: 0.8885\n",
      "Accuracy after query 23: 0.9013\n",
      "Accuracy after query 24: 0.9102\n",
      "Accuracy after query 25: 0.9053\n",
      "Accuracy after query 26: 0.8989\n",
      "Accuracy after query 27: 0.9087\n",
      "Accuracy after query 28: 0.9083\n",
      "Accuracy after query 29: 0.9118\n",
      "Accuracy after query 30: 0.9123\n",
      "Accuracy after query 31: 0.9150\n",
      "Accuracy after query 32: 0.9092\n",
      "Accuracy after query 33: 0.9107\n",
      "Accuracy after query 34: 0.9214\n",
      "Accuracy after query 35: 0.9195\n",
      "Accuracy after query 36: 0.9286\n",
      "Accuracy after query 37: 0.9160\n",
      "Accuracy after query 38: 0.9236\n",
      "Accuracy after query 39: 0.9253\n",
      "Accuracy after query 40: 0.9190\n",
      "Accuracy after query 41: 0.9275\n",
      "Accuracy after query 42: 0.9273\n",
      "Accuracy after query 43: 0.9286\n",
      "Accuracy after query 44: 0.9345\n",
      "Accuracy after query 45: 0.9339\n",
      "Accuracy after query 46: 0.9329\n",
      "Accuracy after query 47: 0.9305\n",
      "Accuracy after query 48: 0.9408\n",
      "Accuracy after query 49: 0.9365\n",
      "Accuracy after query 50: 0.9385\n",
      "Accuracy after query 51: 0.9248\n",
      "Accuracy after query 52: 0.9438\n",
      "Accuracy after query 53: 0.9446\n",
      "Accuracy after query 54: 0.9468\n",
      "Accuracy after query 55: 0.9420\n",
      "Accuracy after query 56: 0.9456\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(create_keras_model)\n",
    "uniform_perf_hist = active_learning_procedure(uniform,\n",
    "                                              X_test,\n",
    "                                              y_test,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              estimator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
